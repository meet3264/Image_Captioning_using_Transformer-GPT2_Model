# Image Captioning Using Transformer-GPT2 Model

Image captioning is a task that combines computer vision and natural language processing to generate textual descriptions for images. This project uses a Transformer-based architecture, leveraging GPT-2 for generating fluent and contextually accurate captions.

## Table of Contents
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Acknowledgments](#acknowledgments)
 

## Features
- **Transformer-Based Model**: Utilizes transformer architecture instead of traditional CNN-RNN models.
- **Pretrained Models**: Option to load pretrained models or train from scratch.
- **Evaluation Metrics**: Includes metrics to assess the accuracy of generated captions.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/meet3264/Image_Captioning_using_Transformer-GPT2_Model.git
   cd Image_Captioning_using_Transformer-GPT2_Model
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```
## Usage

1. **Dataset Preparation**: Place your dataset in the `coco_dataset` folder. This project uses COCO2014 as an example dataset, but you can use any similar dataset.
   
### Training

The model can be trained from scratch or fine-tuned on a custom dataset. The `train_transformer.py` script handles the training process, including data loading, model initialization, and checkpoint saving.

2. **Running the Training Script**:
   To start training the model, use the following command:
   ```bash
   python train_transformer.py
   ```
### Evaluation

The evaluation process helps to measure the quality and accuracy of the captions generated by the model. In this project, common image captioning metrics such as **BLEU** (Bilingual Evaluation Understudy) score and **CIDEr** (Consensus-based Image Description Evaluation) are used to quantify the performance of the model.

3. **Running the Evaluation Script**: 
   To evaluate the model on a test dataset, run the following command:
   ```bash
   python evaluation.py
   ```

### Testing

Once the model is trained, you can test it by generating captions for new images. The `test.py` script allows you to load an image, pass it through the model, and output the generated caption.

4. **Running the Test Script**:
   Use the following command to test the model with new images:
   ```bash
   python test.py --image_path /path/to/your/image.jpg
   ```

### Server & Streamlit

The project includes a `server.py` script to deploy the trained model as a web API, allowing you to generate captions for images via HTTP requests. This makes it easy to integrate the model into web applications or other services.

5. **Starting the Server**:
   To start the server, use the following command:
   ```bash
   python server.py
   ```
6. **Starting the Streamlit app**:
   To start the app, use the following command:
   ```bash
   streamlit run app.py
   ```
## Project Structure

The project directory is organized as follows:
```
├── coco_dataset          # Dataset directory (e.g., COCO)
├── model                 # Directory for saving trained models
├── notebooks             # Jupyter notebooks for experimentation and visualization
├── app.py                # Application script for API deployment
├── evaluation.py         # Evaluation script for model performance
├── requirements.txt      # Dependencies for the project
├── server.py             # Server script to deploy the model
├── test.py               # Script for testing the model on new images
├── train_transformer.py  # Training script for transformer-based image captioning
└── README.md             # Project documentation

```
## Model URL
The trained model can be downloaded from the following link: 
<a href="https://drive.google.com/uc?id=1N_gmx8tJtlV-UC6uTUIFEEWDwRdWngI-">Model Download</a>

## Acknowledgments

Special thanks to the open-source community and the contributors who helped build the libraries and datasets used in this project.
